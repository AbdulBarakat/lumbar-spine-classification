{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8391a5d3"
      },
      "source": [
        "\n",
        "# Lumbar Spine Degeneration Classification with ResNet-50\n",
        "\n",
        "This project applies deep learning to classify lumbar spine degeneration from MRI images using a fine-tuned ResNet-50 model.\n",
        "The goal is to assist in automated diagnostic processes, which can help medical professionals by providing preliminary assessments.\n",
        "\n",
        "## Features\n",
        "- **Dataset**: RSNA Lumbar Spine Dataset.\n",
        "- **Model**: Pre-trained ResNet-50, fine-tuned for multi-class classification.\n",
        "- **Objective**: To classify spinal degeneration into distinct categories.\n",
        "\n",
        "## Objectives:\n",
        "1. **Data Understanding and Preparation**: Load and preprocess MRI images for model training.\n",
        "2. **Model Development**: Use a pre-trained ResNet-50 as the backbone and fine-tune it for classification.\n",
        "3. **Training and Optimization**: Train the model using advanced optimization techniques to handle data imbalance.\n",
        "4. **Evaluation**: Measure performance using appropriate metrics and analyze results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQJsKwOQBREX"
      },
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "### Steps:\n",
        "1. **Load Dataset**: Import MRI images and associated labels.\n",
        "2. **Handle Class Imbalance**: Apply oversampling, undersampling, or data augmentation to balance classes.\n",
        "3. **Data Augmentation**: Use transformations such as rotation, flipping, and scaling to enhance generalization.\n",
        "4. **Prepare for Model Input**: Resize images to match the input size of ResNet-50 and normalize pixel values.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWbOccNf4UxW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-VEkfNZOtuT"
      },
      "source": [
        "From a different run, I found the files that do not have images in them but are called out in the dataset and saved the in a csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCxBeKf5s0Hn"
      },
      "outputs": [],
      "source": [
        "missing_files_df = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/RSNA2024LumbarSpineDegenerativeClassification/missing_files.csv')\n",
        "missing_files = missing_files_df['study_id'].to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD7UKIV1O-uy"
      },
      "source": [
        "Load the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjjwL44A4UxY"
      },
      "outputs": [],
      "source": [
        "#Load the csv file to data frames\n",
        "train_labels = pd.read_csv(\"/content/drive/MyDrive/ColabNotebooks/RSNA2024LumbarSpineDegenerativeClassification/train.csv\")\n",
        "train_coordinates = pd.read_csv(\"/content/drive/MyDrive/ColabNotebooks/RSNA2024LumbarSpineDegenerativeClassification//train_label_coordinates.csv\")\n",
        "series_descriptions = pd.read_csv(\"/content/drive/MyDrive/ColabNotebooks/RSNA2024LumbarSpineDegenerativeClassification//train_series_descriptions.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntrHVDwn1f0t"
      },
      "outputs": [],
      "source": [
        "print(\"Lenghth of train_labels:\", train_labels.shape[0])\n",
        "print(\"Lenghth of train_coordinates:\", train_coordinates.shape[0])\n",
        "print(\"Lenghth of series_descriptions:\", series_descriptions.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8HndtoQzOpf"
      },
      "outputs": [],
      "source": [
        "missing_train_labels = train_labels[train_labels.isnull().any(axis=1)]['study_id']\n",
        "missing_train_labels = missing_train_labels.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoaIfz3r2Jkl"
      },
      "outputs": [],
      "source": [
        "#This cell is to find that data that is not shared between the datasets\n",
        "study_id_train_labels = train_labels['study_id'].to_numpy()\n",
        "study_id_train_coordinates = train_coordinates['study_id'].unique()\n",
        "\n",
        "#Find the values in study_id_train_labels that are not in study_id_train_coordinates\n",
        "not_in_coordinates = np.setdiff1d(study_id_train_labels, study_id_train_coordinates)\n",
        "\n",
        "#Find the values in study_id_train_coordinates that are not in study_id_train_labels\n",
        "not_in_labels = np.setdiff1d(study_id_train_coordinates, study_id_train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtBYKCDJPFRO"
      },
      "source": [
        "Drop the cases that have missing data from all data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsBF1jdS3XDX"
      },
      "outputs": [],
      "source": [
        "#drop the data that is not shared from the datasets\n",
        "train_labels = train_labels.drop(train_labels[train_labels['study_id'].isin(not_in_coordinates)].index)\n",
        "train_labels = train_labels.drop(train_labels[train_labels['study_id'].isin(not_in_labels)].index)\n",
        "train_labels = train_labels.drop(train_labels[train_labels['study_id'].isin(missing_train_labels.to_numpy())].index)\n",
        "train_labels = train_labels.drop(train_labels[train_labels['study_id'].isin(missing_files)].index)\n",
        "train_labels = train_labels.reset_index(drop=True)\n",
        "\n",
        "train_coordinates = train_coordinates.drop(train_coordinates[train_coordinates['study_id'].isin(not_in_coordinates)].index)\n",
        "train_coordinates = train_coordinates.drop(train_coordinates[train_coordinates['study_id'].isin(not_in_labels)].index)\n",
        "train_coordinates = train_coordinates.drop(train_coordinates[train_coordinates['study_id'].isin(missing_train_labels.to_numpy())].index)\n",
        "train_coordinates = train_coordinates.drop(train_coordinates[train_coordinates['study_id'].isin(missing_files)].index)\n",
        "train_coordinates = train_coordinates.reset_index(drop=True)\n",
        "\n",
        "print(\"Lenghth of train_labels:\", train_labels.shape[0])\n",
        "print(\"Lenghth of train_coordinates:\", train_coordinates.shape[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71Lh-6FCPMbm"
      },
      "source": [
        "Here, I created a melted dataframe of the train_labels to be able to compare it with the train_coordinates and check for any additional missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_srnwdaXZ-4Q"
      },
      "outputs": [],
      "source": [
        "#Create labels for the images\n",
        "valuevar = train_labels.columns\n",
        "fvaluevar = valuevar.drop('study_id')\n",
        "\n",
        "#Step 1: Melt the dataframe\n",
        "melted_df = pd.melt(train_labels, id_vars=['study_id'],\n",
        "                    value_vars=fvaluevar,\n",
        "                    var_name='condition', value_name='severity')\n",
        "\n",
        "#Step 2: Create the row_id by concatenating study_id and condition\n",
        "melted_df['row_id'] = melted_df['study_id'].astype(str) + '_' + melted_df['condition']\n",
        "\n",
        "#Step 3: Reorder columns, putting row_id at the front\n",
        "final_df = melted_df[['row_id','study_id', 'condition','severity']]\n",
        "final_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG0qQji2Pldn"
      },
      "source": [
        "I manually checked for any addtional missing data and saved them is a csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7uMlqo0HLx1"
      },
      "outputs": [],
      "source": [
        "additional_drop = pd.read_csv(\"/content/drive/MyDrive/ColabNotebooks/RSNA2024LumbarSpineDegenerativeClassification/Addtional_drop.csv\")\n",
        "additional_drop_labels = additional_drop['Drop'].to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OFLaIprPwwh"
      },
      "source": [
        "Sort the data sets to allign all the data correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIxi1NG8IeBa"
      },
      "outputs": [],
      "source": [
        "final_df = final_df.drop(final_df[final_df['row_id'].isin(additional_drop_labels)].index)\n",
        "final_df = final_df.sort_values(by=['study_id', 'condition'], ascending=[True, True])\n",
        "final_df = final_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM3XtJM5N20Q"
      },
      "outputs": [],
      "source": [
        "final_df2 = final_df.drop(columns=['row_id','condition', 'study_id'])\n",
        "img_labels = final_df2.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvrqqQdIgQmW"
      },
      "outputs": [],
      "source": [
        "np.save(\"/content/drive/MyDrive/ColabNotebooks/RSNA2024LumbarSpineDegenerativeClassification/train_images_labels.np\", img_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDG1RUqAM7a7"
      },
      "outputs": [],
      "source": [
        "train_coordinates = train_coordinates.sort_values(by=['study_id','condition','level'], ascending=[True, True, True])\n",
        "train_coordinates = train_coordinates.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmUI9hQoP6MA"
      },
      "source": [
        "Check if the datasets have the same size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joxdOCNCwT_P"
      },
      "outputs": [],
      "source": [
        "print(\"Labels length:\", len(final_df))\n",
        "print(\"Coordinates length:\", len(train_coordinates))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24EmqpH3QBhB"
      },
      "source": [
        "Extract the images from their files and save them in an array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIpdKTPE4UxY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pydicom\n",
        "import cv2\n",
        "\n",
        "#Function to load the images\n",
        "def load_images(image_path, img_size=(224,224)):\n",
        "    dicom = pydicom.dcmread(image_path)\n",
        "    img = dicom.pixel_array\n",
        "    img = cv2.resize(img, img_size) #resize image for the model\n",
        "    img = img/np.max(img) #normalize image pixels\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4EtejOH4UxY"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "#array for images\n",
        "train_images = []\n",
        "\n",
        "#Initialize a list to record study_ids with missing files\n",
        "missing_files = []\n",
        "\n",
        "#Loop over each study and use the serie_id and instance_number to load the image\n",
        "for i, study_id in tqdm(enumerate(train_coordinates['study_id']), total=len(train_coordinates)):\n",
        "    # Construct the file path\n",
        "    path = \"/content/drive/MyDrive/ColabNotebooks/RSNA2024LumbarSpineDegenerativeClassification/train_images/\" + str(study_id) + \"/\" + str(train_coordinates.iloc[i][1]) + \"/\" + str(train_coordinates.iloc[i][2]) + \".dcm\"\n",
        "    train_images.append(load_images(path))\n",
        "\n",
        "np.save(\"/content/drive/MyDrive/ColabNotebooks/RSNA2024LumbarSpineDegenerativeClassification/train_images_array.np\", train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jeb64w_0OPZR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "#train_images = np.load(\"/content/drive/MyDrive/ColabNotebooks/RSNA2024LumbarSpineDegenerativeClassification/train_images_array.np.npy\")\n",
        "labels = np.load(\"/content/drive/MyDrive/ColabNotebooks/RSNA2024LumbarSpineDegenerativeClassification/train_images_labels.np.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca2iYZuV7CRV"
      },
      "outputs": [],
      "source": [
        "train_images = np.expand_dims(train_images, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhWKljGADf6f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "# Define the path to the saved array (assumes the images are already saved in Google Drive)\n",
        "image_array_path = \"/content/drive/MyDrive/ColabNotebooks/RSNA2024LumbarSpineDegenerativeClassification/train_images_array.np.npy\"\n",
        "\n",
        "# Create a memory-mapped array with the target shape (35957, 224, 224, 3)\n",
        "memmap_file = np.memmap('train_images_memmap.dat', dtype='float32', mode='w+', shape=(35957, 224, 224, 3))\n",
        "\n",
        "# Load the original array (in chunks if possible) and copy it into the memmap array, while adding the third channel\n",
        "original_images = np.load(image_array_path)  # Loading the (35957, 224, 224) array\n",
        "\n",
        "for i in range(original_images.shape[0]):\n",
        "    # Add the third channel (grayscale images) and store it in the memmap array\n",
        "    memmap_file[i] = np.stack([original_images[i]] * 3, axis=-1)\n",
        "\n",
        "# Flush the changes to disk\n",
        "memmap_file.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU6hc4z0QNnO"
      },
      "source": [
        "Use data data augmentation to increase the diversity of the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xcE5sinOhj9"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Fit the generator to the training images\n",
        "datagen.fit(train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAjoORK8_Lef"
      },
      "outputs": [],
      "source": [
        "#Apply elastic deformations\n",
        "from imgaug import augmenters as iaa\n",
        "seq = iaa.Sequential([iaa.ElasticTransformation(alpha=50, sigma=5)])\n",
        "#Fit the deformations to the training images\n",
        "train_images = seq(images=train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4Lto82t4UxZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(train_images[62])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLOMvoS_QUXv"
      },
      "source": [
        "Split the data for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doSfj7wMQAuA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knqQ08weD1La"
      },
      "outputs": [],
      "source": [
        "# Load the memory-mapped array (without loading it entirely into RAM)\n",
        "memmap_file = np.memmap(\"/content/drive/MyDrive/ColabNotebooks/RSNA2024LumbarSpineDegenerativeClassification/train_images_memmap.dat\", dtype='float32', mode='r', shape=(35957, 224, 224, 3))\n",
        "\n",
        "# Use this array during training\n",
        "print(memmap_file.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XblpaTpTE0ES"
      },
      "outputs": [],
      "source": [
        "train_images = np.array(memmap_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW0dDM_RQYek"
      },
      "outputs": [],
      "source": [
        "labels = np.load(\"/content/drive/MyDrive/ColabNotebooks/RSNA2024LumbarSpineDegenerativeClassification/train_images_labels.np.npy\", allow_pickle=True)\n",
        "\n",
        "label_mapping = {\"Normal/Mild\": 0, \"Moderate\": 1, \"Severe\": 2}\n",
        "\n",
        "labels = np.array([label_mapping[label[0]] for label in labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAx3mSfqCQSG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "train_images = preprocess_input(train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUAZWGkX_kr1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_images, labels, test_size=0.2, random_state=42, stratify=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvXtS4PRjDPs"
      },
      "outputs": [],
      "source": [
        "# Convert the labels to categorical format (if necessary)\n",
        "y_train = to_categorical(y_train, num_classes=3)\n",
        "y_val = to_categorical(y_val, num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5PbBeRJRh1J"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "del memmap_file\n",
        "del train_images\n",
        "del labels\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7vC8ieHIXIA"
      },
      "outputs": [],
      "source": [
        "#Create temp storage path\n",
        "temp_storage_path = '/content/drive/MyDrive/ColabNotebooks/RSNA2024LumbarSpineDegenerativeClassification/training_temp/'\n",
        "\n",
        "#Create the directory if it doesn't exist\n",
        "os.makedirs(temp_storage_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGb-XYUNBLpk"
      },
      "source": [
        "## Model Architecture\n",
        "\n",
        "The model leverages **ResNet-50**, a widely used pre-trained convolutional neural network for feature extraction.\n",
        "Key modifications include:\n",
        "- Replacing the final fully connected layer to match the number of output classes.\n",
        "- Adding dropout and L2 regularization to mitigate overfitting.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Optimization\n",
        "\n",
        "### Steps:\n",
        "1. **Loss Function**: Use focal loss to handle class imbalance effectively by prioritizing hard-to-classify samples.\n",
        "2. **Optimizer**: Employ the Adam optimizer for adaptive learning rate adjustments.\n",
        "3. **Early Stopping and Checkpoints**: Monitor validation loss to save the best-performing model.\n",
        "4. **Augmented Training**: Train the model on the augmented dataset to improve robustness.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "xvGLY0GTjELZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRbz-xMwDaEo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#Define Focal Loss function to handle class imbalance\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def focal_loss(gamma=2., alpha=[0.2, 0.5, 0.7]):\n",
        "  @tf.keras.utils.register_keras_serializable()\n",
        "  def focal_loss_fixed(y_true, y_pred):\n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    cross_entropy = -y_true * tf.math.log(y_pred)\n",
        "    loss = alpha * tf.math.pow(1 - y_pred, gamma) * cross_entropy\n",
        "    return tf.reduce_mean(loss, axis=-1)\n",
        "  return focal_loss_fixed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xq9GDTXdHWV"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "#Compute class weights\n",
        "y_train_classes = np.argmax(y_train, axis=1)\n",
        "class_weights_array = class_weight.compute_class_weight('balanced', classes=np.unique(y_train_classes), y=y_train_classes)\n",
        "\n",
        "#Convert the NumPy array to a dictionary\n",
        "class_weights = dict(enumerate(class_weights_array))\n",
        "\n",
        "\n",
        "#For Focal Loss\n",
        "total_examples = 28140 + 5660 + 2157\n",
        "num_classes = 3\n",
        "\n",
        "alpha = [\n",
        "    total_examples / (num_classes * 28140),  # Class 0\n",
        "    total_examples / (num_classes * 5660),   # Class 1\n",
        "    total_examples / (num_classes * 2157)    # Class 2\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbTnD1jO_iBX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#Load the base ResNet50 model with pre-trained ImageNet weights, excluding the top layer\n",
        "base_model = tf.keras.applications.ResNet50(\n",
        "    weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "#Freeze the layers of the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "#Add custom layers for fine-tuning\n",
        "x = base_model.output  # Output of ResNet50 model\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)  # Convert feature maps to a single vector per image\n",
        "x = tf.keras.layers.Dense(1024, activation='relu')(x)  # Dense layer with ReLU activation , kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
        "x = tf.keras.layers.BatchNormalization()(x)  # Add Batch Normalization\n",
        "x = tf.keras.layers.Dropout(0.3)(x)  # Dropout for 3rd dense layer\n",
        "output = tf.keras.layers.Dense(3, activation='softmax')(x)  # Final output layer with 3 classes\n",
        "\n",
        "#Create the full model\n",
        "model = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "#Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpFe2kKtStBD"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#Data augmentation and loading using ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=20,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "#Prepare the data generators\n",
        "train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
        "#val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)\n",
        "\n",
        "val_datagen.fit(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnyrXkwGStr-"
      },
      "outputs": [],
      "source": [
        "#Save the model after every epoch in the temp storage file\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    temp_storage_path + 'model_checkpoint_adam.keras',\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#Reduce learning rate when a metric has stopped improving\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n",
        "\n",
        "#Early stopping to prevent overfitting\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=10, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPz-pZq8zQUG"
      },
      "outputs": [],
      "source": [
        "#Fine tune the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=(X_val, y_val),\n",
        "    steps_per_epoch= len(X_train) // batch_size,\n",
        "    epochs=50,\n",
        "    validation_steps= len(X_val) // batch_size,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[checkpoint, reduce_lr, early_stopping],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0peKnpzEz-wk"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/ColabNotebooks/RSNA2024LumbarSpineDegenerativeClassification/freeze_resnet50_mri_model_Adam.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation and Metrics\n",
        "\n",
        "### Metrics:\n",
        "- **Accuracy**: Measure overall classification performance.\n",
        "- **Precision, Recall, F1-Score**: Assess performance on individual classes, particularly imbalanced ones.\n",
        "- **Confusion Matrix**: Visualize model performance across all classes.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "4e0mbEQkjKq3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2wtB7s_N1uj"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "score = model.evaluate(X_val, y_val)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBW67jIpGRY6"
      },
      "outputs": [],
      "source": [
        "#Unfreeze the last 20 layers\n",
        "for layer in base_model.layers[-20:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "#Recompile the model with lower lr\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Save the model after every epoch in the temp storage file\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    temp_storage_path + 'model_checkpoint_adam.keras',\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#Reduce learning rate when a metric has stopped improving\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7, verbose=1)\n",
        "\n",
        "#Early stopping to prevent overfitting\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=10, restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPBJUovCznrI"
      },
      "outputs": [],
      "source": [
        "#Fine tune the model\n",
        "history_tuned = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=(X_val, y_val),\n",
        "    steps_per_epoch= len(X_train) // batch_size,\n",
        "    epochs=50,\n",
        "    validation_steps= len(X_val) // batch_size,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[checkpoint, reduce_lr, early_stopping],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knvyJ2IGG267"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "model.save('/content/drive/MyDrive/ColabNotebooks/RSNA2024LumbarSpineDegenerativeClassification/unfreeze_resnet50_mri_model_Adam.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wELwokEbxBH"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "score = model.evaluate(X_val, y_val)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results and Analysis  \n",
        "\n",
        "**1. Training and Validation Trends**  \n",
        "- The model achieved high test accuracy, indicating effective learning on the training data.  \n",
        "- However, the validation accuracy was significantly lower, suggesting the model might not generalize well to unseen data.  \n",
        "\n",
        "**2. Challenges**  \n",
        "- **Class Bias**: Despite balancing the dataset, the model exhibited a strong bias toward the majority class (Class 0). This may stem from the inherent complexity of minority class patterns or insufficiently diverse augmentations.  \n",
        "- **Overfitting**: The disparity between training/test accuracy and validation accuracy hints at potential overfitting to the training data.  \n",
        "\n",
        "**3. Visualizations and Metrics**  \n",
        "- **Loss Curves**: The training loss consistently decreased, but the validation loss showed fluctuations, further supporting the overfitting concern.  \n",
        "- **Confusion Matrix**: The model predicted Class 0 with high confidence while misclassifying minority classes (Class 1 and Class 2) more frequently.  \n",
        "\n",
        "**Key Observations**:  \n",
        "- Precision and recall for the majority class were notably higher than for the minority classes.  \n",
        "- Validation performance indicates the need for additional regularization techniques or improved data representation for minority classes.  \n",
        "\n",
        "**Summary**:  \n",
        "The model's bias toward the majority class highlights a need for further experimentation. Advanced techniques such as focal loss tuning, minority class oversampling with diverse augmentations, or even semi-supervised learning could address this issue. Additionally, incorporating cross-validation might provide better insights into generalization performance.  \n"
      ],
      "metadata": {
        "id": "fxeB6-KtjZty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion and Future Work\n",
        "\n",
        "This project successfully demonstrates the application of deep learning to classify lumbar spine degeneration.\n",
        "Future improvements could include:\n",
        "- Exploring additional architectures (e.g., EfficientNet or Vision Transformers).\n",
        "- Applying semi-supervised learning to leverage unlabeled data.\n",
        "- Conducting hyperparameter tuning to further optimize performance.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0IdaC4jRjgaN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bea799c7"
      },
      "source": [
        "\n",
        "## Usage\n",
        "\n",
        "1. Clone the repository:\n",
        "   ```bash\n",
        "   git clone https://github.com/AbduBarakat/lumbar-spine-classification.git\n",
        "   cd lumbar-spine-classification\n",
        "   ```\n",
        "\n",
        "2. Install dependencies:\n",
        "   ```bash\n",
        "   pip install -r requirements.txt\n",
        "   ```\n",
        "\n",
        "3. Place your dataset in the `data/` folder.\n",
        "\n",
        "4. Run the notebook:\n",
        "   ```bash\n",
        "   jupyter notebook lumbar_spine_classification.ipynb\n",
        "   ```\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "databundleVersionId": 8561470,
          "sourceId": 71549,
          "sourceType": "competition"
        },
        {
          "datasetId": 5714107,
          "sourceId": 9410157,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30762,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}